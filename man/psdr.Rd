% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fn_psdr.R
\name{psdr}
\alias{psdr}
\title{Unified principal sufficient dimension reduction methods}
\usage{
psdr(
  x,
  y,
  loss = "svm",
  h = 10,
  lambda = 1,
  eps = 1e-05,
  max.iter = 100,
  eta = 0.1,
  plot = FALSE
)
}
\arguments{
\item{x}{input matrix, of dimension \code{nobs} x \code{nvars}; each row is an observation vector. Requirement: \code{nvars}>1; in other words, \code{x} should have 2 or more columns.}

\item{y}{response variable, either can be continuous variable or (+1,-1) coded binary response vector.}

\item{loss}{pre-specified loss functions belongs to "svm", "logit","l2svm","wsvm", and etc., and user-defined loss function object also can be used formed by inside double (or single) quotation mark. Default is 'svm'.}

\item{h}{the number of slices and probabilities equally spaced in \eqn{(0,1)}. Default value is 10.}

\item{lambda}{the cost parameter for the svm loss function. The default value is 1.}

\item{eps}{the threshold for stopping iteration with respect to the magnitude of the change of the derivative. The default value is 1.0e-5.}

\item{max.iter}{maximum iteration number for the optimization process. default value is 100.}

\item{eta}{learning rate for the gradient descent algorithm. The default value is 0.1.}

\item{plot}{If \code{TRUE} then it produces scatter plots of \eqn{Y} versus \eqn{\hat{B^{\top}}_{j}\mathbf{X}}. \eqn{j} can be specified by the user with \eqn{j=1} as a default. The default is FALSE.}
}
\value{
An object with S3 class "psdr". Details are listed below.
\item{\code{Mn}}{The estimated working matrix, which is obtained by the cumulative
outer product of the estimated parameters over the slices. It will not print out, unless it is called manually.}
\item{\code{evalues}}{Eigenvalues of the working matrix \eqn{Mn}}
\item{\code{evectors}}{Eigenvectors of the \eqn{Mn}, the first leading \eqn{d} eigenvectors consists
the basis of the central subspace}
}
\description{
A unified and user-friendly \proglang{R} package for applying the principal sufficient dimension reduction methods for both linear and nonlinear, and regression and classification context.
The package has an extendable power by varying loss functions for the SVM, even for an user-defined arbitrary function,
unless those are convex and differentiable everywhere over the support.
Details on \code{loss} option
The argument \code{loss} determines a specific loss function for SVM and the corresponding SDR method.
It is important to remark that the convexity of the loss function is the only requirement for the unbiasedness of the PSVM,
and this naturally leads a generalized version of PSVM, which we call the principal machine (PM).
For example, \code{loss="lssvm"} means that the user can do SDR with least square SVM, \code{loss="asls"} is for asymmetric least square loss, and
\code{loss="wlogit"}, which means weighted logistic loss. Entire list of loss functions is found at the \url{https://CRAN.R-project.org/package=psvmSDR}.
Not only function \code{psdr} includes popular loss functions, but also, it is designed for working with user defined arbitrary convex loss function that is claimed through the argument \code{loss}.
Two examples of the usage of user-defined losses are presented below (\code{u} represents a margin):
}
\details{
\code{mylogit <- function(u, ...) log(1+exp(-u))},

\code{myls <- function(u, type="r", ...) u^2}.

Argument \code{u} is a function variable  (any character is possible) and \code{type} determines either margin type (\code{type="m"}) or residual type (\code{type="r"}) method. \code{type="m"} is a default.
Users have to change \code{type="r"}, when applying residual type loss.
Any additional parameters of the loss can be specified via \code{...} argument.
}
\examples{
\donttest{
set.seed(1)
n <- 200; p <- 5;
x <- matrix(rnorm(n*p, 0, 2), n, p)
y <-  x[,1]/(0.5 + (x[,2] + 1)^2) + 0.2*rnorm(n)
y.tilde <- sign(y)
obj <- psdr(x, y)
print(obj)
plot(obj, d=2)

obj_wsvm <- psdr(x, y.tilde, loss="wsvm")
plot(obj_wsvm)

mylogit <- function(u, type="m", ...){log(1+exp(-u))}
obj_mylogit <- psdr(x, y, loss="mylogit")
print(obj_mylogit)

##real data: Boston housing data
data(Boston, package='MASS')
attach(Boston)
Boston <- Boston[Boston$crim < 3.2 , -c(4,9)]
X <- as.matrix(Boston[,-12])
Y <- Boston[,"medv"]
p <- ncol(X);
set.seed(1);
rslt <- psdr(X, Y, h=20, lambda=0.1)
plot(rslt)
}
}
\references{
Artemiou, A. and Dong, Y. (2016)
\emph{Sufficient dimension reduction via principal lq support vector machine,
 Electronic Journal of Statistics 10: 783–805}.\cr
 Artemiou, A., Dong, Y. and Shin, S. J. (2021)
\emph{Real-time sufficient dimension reduction through principal least
 squares support vector machines, Pattern Recognition 112: 107768}.\cr
 Kim, B. and Shin, S. J. (2019)
\emph{Principal weighted logistic regression for sufficient dimension
reduction in binary classification, Journal of the Korean Statistical Society 48(2): 194–206}.\cr
 Li, B., Artemiou, A. and Li, L. (2011)
\emph{Principal support vector machines for linear and
nonlinear sufficient dimension reduction, Annals of Statistics 39(6): 3182–3210}.\cr
Soale, A.-N. and Dong, Y. (2022)
\emph{On sufficient dimension reduction via principal asymmetric
 least squares, Journal of Nonparametric Statistics 34(1): 77–94}.\cr
 Wang, C., Shin, S. J. and Wu, Y. (2018)
\emph{Principal quantile regression for sufficient dimension
 reduction with heteroscedasticity, Electronic Journal of Statistics 12(2): 2114–2140}.\cr
 Shin, S. J., Wu, Y., Zhang, H. H. and Liu, Y. (2017)
\emph{Principal weighted support vector machines for sufficient dimension reduction in
 binary classification, Biometrika 104(1): 67–81}. \cr
 Li, L. (2007)
\emph{Sparse sufficient dimension reduction, Biometrika 94(3): 603–613}.
}
\seealso{
\code{\link{psdr_bic}}, \code{\link{rtpsdr}}
}
\author{
Jungmin Shin, \email{jungminshin@korea.ac.kr}, Seung Jun Shin, \email{sjshin@korea.ac.kr}, Andreas Artemiou \email{artemiou@uol.ac.cy}
}
