% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fn_psdr.R
\name{GD.psdr.solver}
\alias{GD.psdr.solver}
\title{Gradient descent based sufficient dimension reduction machine}
\usage{
GD.psdr.solver(
  x,
  y,
  init.theta,
  H,
  lambda,
  h,
  delta,
  eps,
  max.iter = NULL,
  loss,
  a = NULL,
  c = NULL,
  stochastic = FALSE
)
}
\arguments{
\item{x}{data matrix}

\item{y}{either continuous or (+1,-1) typed binary response vector}

\item{init.theta}{initial parameter theta}

\item{H}{the number of slicing}

\item{lambda}{hyperparameter for the loss function}

\item{h}{very small interval for calculating numerical derivatives for a given arbitrary loss function}

\item{delta}{learning rate}

\item{eps}{threshold for stopping iteration with respect to the magnitude of derivative}

\item{max.iter}{maximum iteration number for the optimization process}

\item{loss}{pre-specified loss functions are "logistic", svm","l2svm","lwpsvm", and user-defined loss function object also can be used formed by inside double quotation mark}

\item{a}{hyperparameter for LUM type loss function}

\item{c}{hyperparameter for LUM type loss function}

\item{stochastic}{user's can implement batch stochastic gradient descent method in case of large number of observations. default is set to FALSE.}
}
\value{
An estimated matrix B for a sufficient dimension reduction will be returned
}
\description{
A method for reducing data dimension using gradient descent optimization method
}
\examples{
\donttest{
set.seed(1234)
n <- 300; p <- 5; H <- 10; lambda <- 0.1; eps <- 1.0e-6;
max.iter <- 10; h <- 1.0e-5; delta <- 3*1.0e-1;
init.theta <- rnorm(sd=1,n=p)
x <- matrix(rnorm(n*p, 0, 1), n, p)
err <- rnorm(n, 0, .2)
B <- matrix(0, p, 2)
B[1,1] <- 1; B[2,2] <- 1
x1 <- x \%*\% B[,1]
x2 <- x \%*\% B[,2]
fx <-  x1/(0.5 + (x2 + 1)^2)
y <- fx + err
y.binary <- sign(fx + err)
my.logistic <- function(m,...){
rslt <- log(1+exp(-m))
return(rslt)
}
GD.psdr.solver(x, y, init.theta, H,lambda, h, delta, eps, max.iter, loss="svm")
GD.psdr.solver(x, y, init.theta, H,lambda, h, delta, eps, max.iter, loss="LUM", a=1, c=10^8)
GD.psdr.solver(x, y.binary, init.theta, H, lambda=0.5, h, delta, eps, max.iter, loss="my.logistic")

#wisconsin breast cancer data example
wisc <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data", sep = ",")
x.wisc <- matrix(unlist(wisc[,-c(1,2)]), ncol = 30)
y.wisc <- y.wisc.binary <- 2*as.numeric(as.factor(unlist(wisc[,2]))) - 3
n <- length(y.wisc.binary) #n=569
p <- ncol(x.wisc)  #p=30
init.theta <- rnorm(sd=1,n=p)
wisc.obj <- GD.psdr.solver(x.wisc, y.wisc.binary, init.theta, H=20,lambda=0.1, h=1.0e-6, delta=0.5,
                          eps=10^-4, max.iter=30, loss="wlogistic")
value.lsvm <- wisc.obj$values
lsvm <- round(wisc.obj$vectors,3)
x.lsvm <- x.wisc \%*\% lsvm
plot(x.lsvm[,1], x.lsvm[,2], type = "n", xlab = "1st predictor", ylab  = "2nd predictor")
points(x.lsvm[y.wisc.binary == 1,1], x.lsvm[y.wisc.binary == 1,2], col = 4, pch = "+")
points(x.lsvm[y.wisc.binary != 1,1], x.lsvm[y.wisc.binary != 1,2], col = 2)
}
}
\seealso{
\code{\link{plot.psdr}}
}
\author{
Jungmin Shin, \email{jungminshin@korea.ac.kr}, Seungjun Shin, \email{sjshin@korea.ac.kr}
}
